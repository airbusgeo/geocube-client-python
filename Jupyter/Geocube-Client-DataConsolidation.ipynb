{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geocube Data Consolidation Tutorial\n",
    "\n",
    "-------\n",
    "\n",
    "**Short description**\n",
    "\n",
    "This notebook introduces you to the Geocube Python Client. You will learn how to optimize the data format of the images in the Geocube. This process is called Consolidation.\n",
    "\n",
    "-------\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "-------\n",
    "\n",
    "- Python 3.7\n",
    "- The Geocube Python Client library : https://github.com/airbusgeo/geocube-client-python.git\n",
    "- The Geocube Server & Client ApiKey (for the purpose of this notebook, GEOCUBE_SERVER and GEOCUBE_CLIENTAPIKEY environment variable)\n",
    "\n",
    "- To have done the **Geocube Data Indexation Tutorial** or to have access to a Geocube with data.\n",
    "\n",
    "- **Geocube DataAccess** and **DataIndexation** tutorials are highly recommended\n",
    "\n",
    "-------\n",
    "\n",
    "**Installation**\n",
    "\n",
    "-------\n",
    "\n",
    "Install Python client:\n",
    "\n",
    "```shell\n",
    "pip install --user git+https://github.com/airbusgeo/geocube-client-python.git\n",
    "```\n",
    "\n",
    "Run dockers (example):\n",
    "```shell\n",
    "export STORAGE=$(pwd)\n",
    "\n",
    "docker run --rm --network=host -e PUBSUB_EMULATOR_HOST=localhost:8085 -v $STORAGE:$STORAGE geocube -project geocube-emulator -ingestionStorage=$STORAGE/ingested -dbConnection=postgresql://user:password@localhost:5432/geocube -psEventsTopic events -psConsolidationsTopic consolidations -local -cancelledJobs $STORAGE/cancelled-jobs\n",
    "\n",
    "docker run --rm --network=host -e PUBSUB_EMULATOR_HOST=localhost:8085 -v $STORAGE:$STORAGE geocube-consolidater /consolidater -project geocube-emulator -workdir=/tmp -psEventsTopic events -psConsolidationsSubscription\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Introduction to Consolidation\n",
    "\n",
    "Consolidation is the process of optimizing the data format, the projection and the tiling of the datasets to fit with the needs of the project.\n",
    "Depending on the depth of the timeseries that is usually needed, the size of the tiles requested, the memory requirements, etc, the datasets can be optimized to improve the speed of access to the data or the memory impact.\n",
    "\n",
    "**Consolidation is not mandatory**, but some applications, especially those requiring massive data and deep timeseries, may suffer from poorly formatted images.\n",
    "\n",
    "For instance, consolidation may be a game changer in the following cases (all the more, if the images are retrieved more than once - reprocessing, visualisation):\n",
    "- if the image format is not cloud-optimized (jpeg2000, GeoTiff, ...)\n",
    "- if the images are retrieved as timeseries\n",
    "- if the storage has a high latency per object.\n",
    "- if the datasets are not in the right projection or resolution\n",
    "- if the images are retrieved with low resolutions (creation of overviews)\n",
    "- if the processing requires small tiles (e.g. deep learning)\n",
    "\n",
    "\n",
    "During the consolidation, the datasets will be tiled, reprojected, casted and merged into files optimized for timeseries.\n",
    "\n",
    "\n",
    "## 1 - Connect to the Geocube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Consolidater` is derived from `Client`. It adds functionalities to Consolidate the datasets and handle jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geocube import Consolidater, utils, entities\n",
    "\n",
    "# Define the connection to the server\n",
    "secure = False # in local, or true to use TLS\n",
    "geocube_client_server  = os.environ['GEOCUBE_SERVER']        # e.g. 127.0.0.1:8080 for local use\n",
    "geocube_client_api_key = os.environ['GEOCUBE_CLIENTAPIKEY']  # Usually empty for local use\n",
    "\n",
    "# Connect to the server\n",
    "consolidater = Consolidater(geocube_client_server, secure, geocube_client_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Consolidation parameters\n",
    "The consolidation parameters that describe the data format of the optimized datasets are linked to a variable.\n",
    "- Internal `Dataformat` \n",
    "- `Exponent` for the mapping between internal dataformat and `variable.dformat` (see formula below)\n",
    "- Creation of `Overviews`\n",
    "- `Resampling algorithm` used for reprojection and overviews\n",
    "- `Compression` of the data\n",
    "- `Bands interleave` if the variable is multi-bands\n",
    "\n",
    "\n",
    "For the consolidation process, the external min/max (below: MinOut/MaxOut) are the Min/Max of the variable.\n",
    "For an complete explanation of the internal dataformat and the exponent, see the [Data Indexation Tutorial # Dataset](./Geocube-Client-DataIndexation.ipynb#5---Dataset).\n",
    "<img src=\"images/DataFormatExample.png\" width=800>\n",
    "\n",
    "The consolidation parameters of a variable are configured with `config_consolidation()`. A call to `config_consolidation` will update the consolidation parameters of the variable and it will only affect the future consolidations.\n",
    "\n",
    "Below, the variable is configured to consolidate datasets using `int16` datatype (half the size of the float32 data type of the variable). The variable defines a range of values equals to `float32[0, 1]`. So `[0, 1]` will be internally mapped to `[0, 255]` with an internal nodata equals to -32768."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = consolidater.variable(name=\"RGB\")\n",
    "variable.config_consolidation(\n",
    "    dformat=(\"i2\", -32768, 0, 255), \n",
    "    resampling_alg=entities.Resampling.cubic\n",
    ")\n",
    "\n",
    "print(\"The consolidation process will cast {}[{},{}] to {}[{},{}]\\n\".format(\n",
    "    variable.dformat.dtype,\n",
    "    variable.dformat.min_value,\n",
    "    variable.dformat.max_value,\n",
    "    variable.consolidation_params.dformat.dtype, \n",
    "    variable.consolidation_params.dformat.min_value, \n",
    "    variable.consolidation_params.dformat.max_value))\n",
    "print(variable.name, variable.consolidation_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Layout\n",
    "The datasets will be tiled, reprojected and stacked on a grid defined by a *Layout*.\n",
    "The layout has *external parameters* that define the grid:\n",
    "- `grid_parameters` : dict of parameters, containing at least a grid type (actually only `singlecell` and `regular` are available)\n",
    "- `grid_flags` : list of flags\n",
    "\n",
    "and *internal parameters* that define the internal tiling and the depth of the stacking:\n",
    "- `block_shape`\n",
    "- `max_records` per file\n",
    "\n",
    "The layout must be carefully defined depending on the performance expected in terms of access.\n",
    "The size of the cell of the grid multiplied by the maximum number of the records and the datatype will give the maximum size of the final files.\n",
    "\n",
    "### Regular layout\n",
    "\n",
    "The regular layout defined a regular grid in a given CRS. The `Layout.regular()` function is a shortcut to define a regular grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_name = \"32632_20m\"\n",
    "layout = entities.Layout.regular(\n",
    "    name=layout_name,\n",
    "    crs=\"epsg:32632\",\n",
    "    cell_size=4096,\n",
    "    resolution=20,\n",
    "    block_size=256,\n",
    "    max_records=1000\n",
    ")\n",
    "try:\n",
    "    consolidater.create_layout(layout)\n",
    "    print(\"Layout created\")\n",
    "except utils.GeocubeError as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# Example:\n",
    "aoi = utils.read_aoi(\"inputs/Denmark.json\")\n",
    "cells=consolidater.tile_aoi(aoi, layout_name=layout_name)\n",
    "base = entities.Tile.plot(cells)\n",
    "import geopandas as gpd\n",
    "aoi_gpd = gpd.GeoDataFrame({'id': ['1'], 'geometry': gpd.GeoSeries(aoi, crs='epsg:4326')})\n",
    "aoi_gpd.plot(ax=base, color='None', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-cell layout\n",
    "\n",
    "The single-cell layout defines a grid of one cell in a given CRS. At the beginning of the consolidation, the aoi of all the datasets will be projected and merged in the given crs. The bounds of this aoi give the size of the cell.\n",
    "\n",
    "Single-cell layout can be used to consolidate a bunch of already aligned records, like Sentinel-2 granules.\n",
    "\n",
    "*Be careful with Single-cell Layout as the merged aoi may be very large and caused memory errors.*\n",
    "\n",
    "The `Layout.single_cell()` function is a shortcut to define a single-cell grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"SingleCellUTM32N_20m\"\n",
    "layout = entities.Layout.single_cell(\n",
    "    name=name,\n",
    "    crs=\"epsg:32632\",\n",
    "    resolution=20,\n",
    "    block_size=256,\n",
    "    max_records=1000\n",
    ")\n",
    "try:\n",
    "    consolidater.create_layout(layout)\n",
    "    print(\"Layout created\")\n",
    "except utils.GeocubeError as e:\n",
    "    print(\"Layout already exists\")\n",
    "\n",
    "# Example:\n",
    "aoi = utils.read_aoi(\"inputs/Denmark.json\")\n",
    "cells=consolidater.tile_aoi(aoi, layout_name=name)\n",
    "base = entities.Tile.plot(cells)\n",
    "import geopandas as gpd\n",
    "aoi_gpd = gpd.GeoDataFrame({'id': ['1'], 'geometry': gpd.GeoSeries(aoi, crs='epsg:4326')})\n",
    "aoi_gpd.plot(ax=base, color='None', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available layouts\n",
    "`list_layouts()` function lists all the layouts already defined in the geocube. With the `name_like` argument, the layouts are filtered by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layouts = consolidater.list_layouts(\"\")\n",
    "for l in layouts:\n",
    "    print(l)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Consolidate\n",
    "\n",
    "Consolidation is an asynchronous process that is defined by a *Job*. A Job is a state machine that can be easily canceled or retried at any state.\n",
    "\n",
    "<img src=\"images/ConsolidationProcess.png\" width=800>\n",
    "\n",
    "A consolidation job is defined by a **name**, a **variable**, a **layout** and **records** that can be passed as a list of records id or as filters (tags, from_time, to_time).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get records\n",
    "jobName = f'MyConsolidation{uuid.uuid1()}'\n",
    "records = consolidater.list_records(tags={'source':'tutorial','satellite':'SENTINEL2B','constellation':'SENTINEL2'}, with_aoi=True)\n",
    "layout = consolidater.list_layouts(\"SingleCellUTM32N_20m\")[0]\n",
    "\n",
    "# Get the variable RGB:master\n",
    "rgb = consolidater.variable(name=\"RGB\").instance(\"master\")\n",
    "\n",
    "job = consolidater.consolidate(jobName, rgb, layout, records, execution_level=entities.ExecutionLevel.STEP_BY_STEP_CRITICAL)\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step jobs\n",
    "A job can be done step-by-step (`STEP_BY_STEP_CRITICAL`, `STEP_BY_STEP_MAJOR`, `STEP_BY_STEP_ALL`) or all in a row.\n",
    "When a step is finished, its status changed and it waits for a user action.\n",
    "If a job is in a waiting mode, start the next step by calling `next()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job.refresh())\n",
    "if job.waiting:\n",
    "    job.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View job tasks\n",
    "Once the job has finished preparing the consolidation orders (state `CONSOLIDATION_CREATED`), the tasks can be visualized for control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.refresh()\n",
    "base=job.plot_tasks()\n",
    "aoi=entities.Record.list_to_geodataframe(records)\n",
    "aoi.plot(ax=base)\n",
    "bounds=aoi.total_bounds\n",
    "margin=1\n",
    "base.set_xlim(bounds[0] - margin, bounds[2] + margin)\n",
    "base.set_ylim(bounds[1] - margin, bounds[3] + margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidater.wait_job(job, wait_secs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check consolidation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_params = entities.CubeParams.from_records(\n",
    "    crs           = \"epsg:32632\",\n",
    "    transform     = entities.geo_transform(563087,6195234, 200),\n",
    "    shape         = (128, 128),\n",
    "    instance      = rgb,\n",
    "    records       = records)\n",
    "metadata = consolidater.get_cube_metadata(cube_params)\n",
    "\n",
    "for s in metadata.slices:\n",
    "    for m in s.metadata:\n",
    "        print(f\"{os.path.basename(m.container_uri)}:{m.container_subdir} => {s.record.name}\")\n",
    "\n",
    "images, rs = consolidater.get_cube(cube_params, verbose=False)\n",
    "\n",
    "nbimages=len(images)\n",
    "plt.rcParams['figure.figsize'] = [20, 16]\n",
    "for f in range(0, nbimages):\n",
    "    plt.subplot(math.ceil(nbimages/4), 4, f+1).set_axis_off()\n",
    "    plt.imshow(images[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deletion task\n",
    "If the geocube owns the datasets that are consolidated (`managed=True` when containers are indexed), it will delete the original datasets using a separate, asynchronous job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deletion_job_name = job.deletion_job_from_logs()\n",
    "if deletion_job_name == \"\":\n",
    "    raise Exception(\"Unable to find deletion_job_name from logs. \" + (\"Please wait until the job finishes\" if job.state != 'DONE' else \"Perhaps, there is nothing to delete\"))\n",
    "deletion_job = consolidater.job(deletion_job_name)\n",
    "print(deletion_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retry or cancel a job \n",
    "In case of failure, a job can be retried or canceled.\n",
    "If a job is cancelled, a complete rollback is done to retrieve the original state.\n",
    "\n",
    "Both functions have a `force` parameter that can be used to retry or cancel a job that is stuck for an unexpected reason. Some steps can take a lot of time (in particular `CONSOLIDATION_INPROGRESS`). So be sure that the job is really stuck before calling these functions with the `force` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job.retry(force=False)\n",
    "# job.cancel(force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = consolidater.list_jobs()\n",
    "\n",
    "for job in jobs:\n",
    "    print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Custom grids & Layouts\n",
    "It's possible to define a custom grid as a set of cells.\n",
    "A cell is defined by an ID (the couple (gridName, ID) must be unique), a CRS and geographic coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "from geocube.utils import grid\n",
    "g = geopandas.read_file(\"./inputs/UTM_grid.geojson\")\n",
    "cells = grid.utm(g.ZONE, g.ROW_, g.geometry)\n",
    "\n",
    "print(\"UTM Cell:\", cells[0])\n",
    "try:\n",
    "    consolidater.create_grid(entities.Grid(\"UTM\", \"UTM Grid\", cells))\n",
    "except utils.GeocubeError as e:\n",
    "    print(e.codename + \": \" + e.details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in consolidater.list_grids(\"\"):\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, a Layout can be created and an AOI can be tiled using this grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = entities.Layout(\n",
    "    name=\"UTM_4096_256_20m\",\n",
    "    grid_parameters={\"grid\":\"UTM\", \"resolution\":\"20\"},\n",
    "    grid_flags=[],\n",
    "    block_shape=(256, 256),\n",
    "    max_records=1000\n",
    ")\n",
    "aoi = utils.read_aoi(\"inputs/Denmark.json\")\n",
    "cells=consolidater.tile_aoi(aoi, layout=layout)\n",
    "\n",
    "# Graphical visualization of tiles and AOI\n",
    "import geopandas as gpd\n",
    "gpd.GeoSeries(aoi, crs='epsg:4326').plot(ax=entities.Tile.plot(cells), color='None', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, the cells of the grid can be subdivided using another grid (currently, only regular is supported).\n",
    "In the following example, the UTM grid, is subdivided using a regular grid of size 4096x4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = entities.Layout(\n",
    "    name=\"UTM_4096_256_20m\",\n",
    "    grid_parameters={\"grid\":\"UTM\", \"subgrid\":\"regular\", \"resolution\":\"20\", \"cell_size\": \"4096\"},\n",
    "    grid_flags=[],\n",
    "    block_shape=(256, 256),\n",
    "    max_records=1000\n",
    ")\n",
    "aoi = utils.read_aoi(\"inputs/Denmark.json\")\n",
    "cells=consolidater.tile_aoi(aoi, layout=layout)\n",
    "\n",
    "# Graphical visualization of tiles and AOI\n",
    "import geopandas as gpd\n",
    "gpd.GeoSeries(aoi, crs='epsg:4326').plot(ax=entities.Tile.plot(cells), color='None', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Conclusion\n",
    "In this notebook, you have learnt to consolidate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Benchmark\n",
    "Consolidation of 100 datasets and retrieving GetCube request.\n",
    "The following code creates 100 different datasets (it requires 500Mb of memory), indexes and consolidates them.\n",
    "\n",
    "It can be used to do benchmarks:\n",
    "- Copy the fake data to another storage and change the uris when indexing to benchmark different storages.\n",
    "- Try to add workers or to increase the blockSize of the Server or the Downloader service (args `--workers` and `--gdalBlockSize`) to see the impact on the time of retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "    \n",
    "print('Clean consolidation files')\n",
    "try:\n",
    "    #`Admin` is derived from `Client`. It adds admin functionalities to be used with cautious.\n",
    "    from geocube import Admin\n",
    "    admin = Admin(geocube_client_server, secure, geocube_client_api_key, verbose=False)\n",
    "    records = admin.list_records(tags={'source':'notebook_consolidation'})\n",
    "    admin.admin_delete_datasets(records=records,instances=[],execution_level=entities.ExecutionLevel.SYNCHRONOUS)\n",
    "    admin.delete_records(records=records)\n",
    "except utils.GeocubeError as e:\n",
    "    if e.codename != \"NOT_FOUND\":\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import os\n",
    "\n",
    "os.makedirs('inputs/data', exist_ok=True)\n",
    "\n",
    "with rasterio.open('inputs/consolidation.tif') as ds:\n",
    "    im = ds.read()\n",
    "    profile = ds.profile\n",
    "for i in range(100):\n",
    "    with rasterio.open(f'inputs/data/consolidation{i}.tif', 'w', **profile) as ds:\n",
    "        ds.write(im+i)\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi = utils.read_aoi('inputs/consolidation.json')\n",
    "aoi_id = consolidater.create_aoi(aoi, exist_ok=True)\n",
    "print(\"AOI created with id \"+aoi_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Variable \n",
    "variable_name = \"Geocube_benchmark\"\n",
    "variable = consolidater.create_variable(\n",
    "    name=variable_name,\n",
    "    dformat={\"dtype\":\"u1\", \"no_data\": 220, \"min_value\": 0, \"max_value\": 255},\n",
    "    bands=['GREY'],\n",
    "    description=\"\",\n",
    "    unit=\"\",\n",
    "    resampling_alg=entities.Resampling.bilinear,\n",
    "    exist_ok=True)\n",
    "\n",
    "try:\n",
    "    instance = variable.instantiate(\"master\", {})\n",
    "except utils.GeocubeError as e:\n",
    "    instance = variable.instance(\"master\")\n",
    "    \n",
    "print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "records = consolidater.create_records(aoi_ids=[aoi_id]*100,names=[f'record_consolidation{i}' for i in range(100)],dates=[datetime.now() for _ in range(100)],tags=[{'source':'notebook_consolidation'}]*100)\n",
    "print(f'Records Created: {records}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geocube.entities import DataFormat\n",
    "\n",
    "records = consolidater.list_records(tags={'source':'notebook_consolidation'})\n",
    "\n",
    "# Indexation\n",
    "for i in range(100):\n",
    "    try:\n",
    "        consolidater.index([entities.Container(\n",
    "            uri=f'{cwd}/inputs/data/consolidation{i}.tif',\n",
    "            managed=True,\n",
    "            datasets=[entities.Dataset(records[i], instance)]\n",
    "        )])\n",
    "    except utils.GeocubeError as e:\n",
    "        if e.codename == \"ALREADY_EXISTS\":\n",
    "            print('Datasets already indexed')\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "print('Indexation complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Cube benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import affine\n",
    "import time\n",
    "from geocube import entities\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_cube_benchmark(records):\n",
    "    crs=\"epsg:2154\"\n",
    "    transform = affine.Affine.translation(526988.10, 6704991.01) * affine.Affine.scale(20,20)\n",
    "    width, height = 256, 256\n",
    "    shape = (width, height)\n",
    "    cube_params = entities.CubeParams.from_records(records=records,transform=transform,instance=instance,crs=crs,shape=shape)\n",
    "\n",
    "    consolidater.get_cube_metadata(cube_params).info()\n",
    "    start = time.time()\n",
    "    images, records = consolidater.get_cube(cube_params, compression=0, verbose=False)\n",
    "    print(f\"{len(images)} images downloaded in {time.time() - start}s\")\n",
    "\n",
    "    from geocube.utils import timeseries_to_animation\n",
    "    import numpy as np\n",
    "\n",
    "    imagesu1=[]\n",
    "    for i in images:   \n",
    "        imagesu1.append(i.astype(np.uint8))\n",
    "\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    timeseries_to_animation(imagesu1, \"./outputs/animation.gif\", duration=0.2)\n",
    "\n",
    "    from IPython.display import Image\n",
    "    with open(os.getcwd() + '/outputs/animation.gif','rb') as f:\n",
    "        display(Image(data=f.read(), format='png', width=512, height=512))\n",
    "        \n",
    "def get_overviews_benchmark(records, resolution=200):\n",
    "    crs=\"epsg:2154\"\n",
    "    consolidater.load_aoi(records[0])\n",
    "    cube_params=entities.CubeParams.from_tile(entities.Tile.from_record(records[0], resolution=resolution, crs=crs), instance=instance, records=records)\n",
    "\n",
    "    start = time.time()\n",
    "    imagesOvr, _ = consolidater.get_cube(cube_params, compression=0, verbose=False)\n",
    "    print(f\"{len(imagesOvr)} overviews downloaded in {time.time() - start}s\")    \n",
    "    \n",
    "    plt.imshow(imagesOvr[0][:,:,0], cmap=\"gray\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Cube before consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = consolidater.list_records(tags={'source':'notebook_consolidation'})\n",
    "get_cube_benchmark(records)\n",
    "get_overviews_benchmark(records[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "name = \"SingleCell_L93_20m\"\n",
    "layout = entities.Layout.single_cell(\n",
    "    name=name,\n",
    "    crs=\"epsg:2154\",\n",
    "    resolution=20,\n",
    "    block_size=256,\n",
    "    max_records=1000\n",
    ")\n",
    "try:\n",
    "    consolidater.create_layout(layout)\n",
    "    print(\"Layout created\")\n",
    "except utils.GeocubeError as e:\n",
    "    print(\"Layout already exists\")\n",
    "    \n",
    "variable = consolidater.variable(name=variable_name)\n",
    "variable.config_consolidation(\n",
    "    dformat=(\"i2\", 220, 0, 255),\n",
    "    compression=entities.Compression.LOSSLESS,\n",
    "    overviews_min_size=256\n",
    ")\n",
    "\n",
    "jobName = f'notebook_job_conso_{uuid.uuid1()}'\n",
    "print(jobName)\n",
    "records = consolidater.list_records(tags={'source':'notebook_consolidation'})\n",
    "\n",
    "job = consolidater.consolidate(jobName, variable.instance(\"master\"), layout, records, execution_level=entities.ExecutionLevel.ASYNCHRONOUS)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidation Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job.refresh())\n",
    "consolidater.wait_job(job, wait_secs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GetCube request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = consolidater.list_records(tags={'source':'notebook_consolidation'})\n",
    "get_cube_benchmark(records)\n",
    "get_overviews_benchmark(records[:10], resolution=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
