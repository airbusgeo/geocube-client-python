{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geocube SDK Tutorial\n",
    "\n",
    "-------\n",
    "\n",
    "#### Short description\n",
    "\n",
    "This notebook introduces you to the SDK package with a complete Geocube workflow using the Python Client. You will see a typical workflow and how to parallelize an image processing algorithm.\n",
    "This notebook is in two chapters. The first presents the SDK, the second is an example of a workflow.\n",
    "\n",
    "-------\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "- Same as [First part](Geocube-Client-SDK.ipynb#Requirements)\n",
    "\n",
    "- The url of a [Geocube Ingester](github.com/airbusgeo/geocube-ingester.git) connected to the Geocube Server\n",
    "- A Scihub account (`SCIHUB_USERNAME` and `SCIHUB_PASSWORD` environment variable) - [Installation](https://github.com/airbusgeo/geocube-ingester/blob/main/INSTALL.MD) \n",
    "\n",
    "- It's highly recommended to have done the other [client tutorials](https://github.com/airbusgeo/geocube-client-python/blob/main/Jupyter) and the [ingester tutorial](https://github.com/airbusgeo/geocube-ingester/blob/main/Jupyter).\n",
    "\n",
    "-------\n",
    "\n",
    "#### Table of content\n",
    "\n",
    "- [Part 1 - SDK](Geocube-Client-SDK-1.ipynb#Table-of-content) (Geocube-Client-SDK-1.ipynb)\n",
    "  \n",
    "  \n",
    "- Part 2 - Abnormal change detection\n",
    "  * [Ingestion](#1---Ingestion)\n",
    "  * [Consolidation](#2---Consolidation)\n",
    "  * [Processing](#3---Parallel-processing-of-timeseries)\n",
    "  * [Monitoring](#4---Monitoring-abnormal-changes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from shapely import geometry\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "from geocube import utils, entities, Consolidater, sdk\n",
    "\n",
    "# Define the connection to the server\n",
    "secure = False # in local, or true to use TLS\n",
    "geocube_client_server  = os.environ['GEOCUBE_SERVER']        # e.g. 127.0.0.1:8080 for local use\n",
    "geocube_client_api_key = os.environ['GEOCUBE_CLIENTAPIKEY']  # Usually empty for local use\n",
    "geocube_downloader = os.environ['GEOCUBE_DOWNLOADER']\n",
    "\n",
    "# Define the parameters to connect to the Geocube server\n",
    "connection_params = sdk.ConnectionParams(geocube_client_server, secure, geocube_client_api_key)\n",
    "\n",
    "# Optionally, use a downloader:\n",
    "connection_params.downloader = sdk.ConnectionParams(geocube_downloader)\n",
    "\n",
    "# Connect to the server\n",
    "client = connection_params.new_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abnormal change detection\n",
    "\n",
    "In July 2021, several European countries were affected by severe floods. In particular, on 16 July around Köln in Germany, many rivers overflowed and a major landslide has destroyed a quarry 20km South-West of Köln.\n",
    "\n",
    "We will try to visualize the aftermath of these floods by looking for abnormal changes.\n",
    "\n",
    "Since coherence detects changes, abnormality can be defined as a significant decrease in coherence compared to usual values. It can be computed with a [z-test](https://en.wikipedia.org/wiki/Z-test) using mean and standard-deviation calculated on a year of images (to prevent from seasonal effects).\n",
    "\n",
    "<img src=\"images/AbnormalChangeAlgorithm.png\" width=500>\n",
    "\n",
    "\n",
    "To be more robust, thresholding is done with hysteresis (`skimage.filters.apply_hysteresis_threshold`). Small areas are filtered out with the `min_area` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops_table\n",
    "from skimage import filters\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def abnormal_changes(image, mean, std, z_score_low, z_score_high, min_area):\n",
    "    z_score = ((mean-image)/std)[:,:,0]\n",
    "    abnormal = filters.apply_hysteresis_threshold(z_score, z_score_low, z_score_high)\n",
    "    blobs = label(abnormal)\n",
    "    props = pd.DataFrame(regionprops_table(blobs, properties=['area', 'bbox']))\n",
    "    \n",
    "    patches = []\n",
    "    changes = np.zeros_like(blobs)\n",
    "    for index, blob in props[props.area>min_area].iterrows():\n",
    "        width = blob['bbox-3'] - blob['bbox-1']\n",
    "        height = blob['bbox-2'] - blob['bbox-0']\n",
    "        patches.append((blob['bbox-1'],blob['bbox-0'], width, height))\n",
    "        changes[np.where(blobs==index)] = 1\n",
    "    return patches, changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and standard deviation are computed on a timeseries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(timeseries):\n",
    "    if len(timeseries) == 0:\n",
    "        return None, None\n",
    "    timeseries[np.where(timeseries==0)] = np.nan\n",
    "    std  = np.nanstd(timeseries, axis=0)\n",
    "    mean = np.nanmean(timeseries, axis=0)\n",
    "    return std, mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be run by the Geocube, the process will be done in four steps:\n",
    "1. **Ingestion** in the Geocube of a year-and-half SAR imagery covering the area.\n",
    "2. **Consolidation** of the data to optimize the retrieval of timeseries.\n",
    "3. **Batch processing** of the calculation of statistics on one-year timeseries (mean and standard deviation with `compute_statistics()`). Results will be stored in the Geocube and used as reference.\n",
    "4. **Monitoring** of the area, to detect abnormal changes on the remaining half of the year (including the floods) with `abnormal_changes()`.\n",
    "\n",
    "\n",
    "<img src=\"images/Process.png\" width=800>\n",
    "\n",
    "For the purpose of this notebook, only a small area around Köln will be processed, but it can be scaled up to the whole Germany or Europe for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dateutil import parser\n",
    "\n",
    "AoiPath=\"inputs/koln.json\"\n",
    "\n",
    "with open(AoiPath, \"r\") as f:\n",
    "    aoiJson = json.load(f)\n",
    "    \n",
    "YearRef = datetime(parser.parse(aoiJson[\"start_time\"]).year, 1, 1)\n",
    "YearCur = datetime(parser.parse(aoiJson[\"end_time\"]).year, 1, 1)\n",
    "CRS = aoiJson[\"graph_config\"][\"projection\"]\n",
    "Resolution = aoiJson[\"graph_config\"][\"resolution\"]\n",
    "RecordTags = aoiJson[\"record_tags\"]\n",
    "RecordTags[\"constellation\"] = \"SENTINEL1\"\n",
    "VariableName=\"CoherenceVV\"\n",
    "InstanceName=\"master\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Ingestion\n",
    "<img src=\"images/1.Ingestion.png\" width=800>\n",
    "The area, the time interval and all the processing parameters are defined in a JSON.\n",
    "\n",
    "This JSON will be sent to the Ingester-Workflow.\n",
    "See [Ingester](https://github.com/airbusgeo/geocube-ingester/) and [Ingestion Notebook](https://github.com/airbusgeo/geocube-ingester/blob/main/Jupyter/Geocube-Ingester-Demo.ipynb) for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(aoiJson, indent=4))\n",
    "\n",
    "# Display AOI\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "aoi = utils.read_aoi(AoiPath)\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "base = world.plot(color='lightgrey', edgecolor='white')\n",
    "gpd.GeoSeries(aoi, crs='epsg:4326').plot(ax=base, edgecolor='black')\n",
    "plt.axis([aoi.bounds[0]-5, aoi.bounds[2]+5, aoi.bounds[1]-3, aoi.bounds[3]+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_server = os.environ.get('GEOCUBE_INGESTER_WORKFLOW')\n",
    "!curl -F \"area=@{AoiPath}\" {workflow_server}/catalog/aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s {workflow_server}/aoi/Koln/dot > outputs/Koln.dot\n",
    "\n",
    "import graphviz\n",
    "dot = graphviz.Source.from_file('Koln.dot', directory=\"outputs\")\n",
    "filename=dot.render(format='png')\n",
    "from IPython.display import Image\n",
    "with open(os.path.join(os.getcwd(), filename),'rb') as f:\n",
    "    display(Image(data=f.read(), format='png', width=1024, height=1024))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingestion workflow\n",
    "    \n",
    "<img src=\"images/Ingestion.dot.png\" width=800>\n",
    "\n",
    "This process may last several hours or days, depending on the number, the power and the internet bandwidth of the processing machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Consolidation\n",
    "\n",
    "<img src=\"images/2.Consolidation.png\" width=800>\n",
    "\n",
    "The consolidation step will rewrite the datasets so that their retrieval as tiled-timeseries is more efficient. It's not mandatory, but it is very interesting in many use-cases (more details in the [Consolidation Notebook](https://github.com/airbusgeo/geocube-client-python/blob/main/Jupyter/Geocube-Client-DataConsolidation.ipynb#0---Introduction-to-Consolidation)).\n",
    "\n",
    "### Layout\n",
    "For this example, the timeseries will be processed using 256x256 tiles in the native CRS and resolution.\n",
    "As the AOI is quite small, there will not have a lot of consolidation tasks (1 to 4 maximum).\n",
    "\n",
    "The consolidation process will prepare the files to be requested as is, using a block size of 256px and a cell size of 4096px and less than 200 records. It means that the datasets will be stored as (4096,4096,200) arrays (this determines the maximum size of the file), with chunks of 256x256 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geocube import Consolidater\n",
    "consolidater = Consolidater(geocube_client_server, secure, geocube_client_api_key)\n",
    "\n",
    "layout_name = f\"{CRS}_{Resolution}m\"\n",
    "cell_size   = 8192\n",
    "layout = entities.Layout.regular(\n",
    "    name=layout_name,\n",
    "    crs=CRS,\n",
    "    cell_size=cell_size,\n",
    "    resolution=Resolution,\n",
    "    block_size=256,\n",
    "    max_records=200\n",
    ")\n",
    "\n",
    "try:\n",
    "    consolidater.create_layout(layout)\n",
    "    print(\"Layout created\")\n",
    "except utils.GeocubeError as e:\n",
    "    print(e)\n",
    "    \n",
    "aoi = utils.read_aoi(AoiPath)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "tiles = consolidater.tile_aoi(aoi, crs=CRS, resolution=Resolution, shape=(cell_size, cell_size))\n",
    "gpd.GeoSeries(aoi, crs='epsg:4326').plot(ax=entities.Tile.plot(tiles), color=\"None\", edgecolor='black')\n",
    "plt.xlim([aoi.bounds[0]-3, aoi.bounds[2]+3])\n",
    "plt.ylim([aoi.bounds[1]-1, aoi.bounds[3]+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Records covering the area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi     = utils.read_aoi(AoiPath)\n",
    "records = consolidater.list_records(aoi=aoi, tags=RecordTags, with_aoi=True)\n",
    "\n",
    "print('---------------------------------')\n",
    "print('{} records found'.format(len(records)))\n",
    "print('---------------------------------')\n",
    "\n",
    "gpdrecords = entities.Record.list_to_geodataframe(records)\n",
    "base=gpdrecords.plot(alpha=0.005)\n",
    "gpd.GeoSeries(aoi, crs='epsg:4326').plot(ax=base, color=\"None\", edgecolor='black')\n",
    "plt.axis(gpdrecords.total_bounds[[0,2,1,3]]+[-0.3, 0.3, -0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidation job\n",
    "Once the records and the layout have been defined, the consolidation process can be started.\n",
    "It's an asynchronous process that takes minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "jobName = \"ConsolidationKolnCohVV2\"\n",
    "\n",
    "# Get the variable\n",
    "v = consolidater.variable(VariableName)\n",
    "v.config_consolidation(entities.DataFormat(\"int16\", 0, 1000, -32768), resampling_alg=entities.Resampling.cubic)\n",
    "vi = v.instance(InstanceName)\n",
    "\n",
    "# Start the consolidation\n",
    "try:\n",
    "    job = consolidater.consolidate(jobName, vi, layout_name, records, execution_level=entities.ExecutionLevel.STEP_BY_STEP_CRITICAL)\n",
    "except utils.GeocubeError as e:\n",
    "    if not e.is_already_exists():\n",
    "        raise\n",
    "    job = consolidater.job(jobName)\n",
    "\n",
    "# Display tasks\n",
    "time.sleep(10)\n",
    "try:\n",
    "    base = job.plot_tasks()\n",
    "    gpdrecords = entities.Record.list_to_geodataframe(records)\n",
    "    gpdrecords.plot(ax=base, alpha=0.005)\n",
    "    gpd.GeoSeries(aoi, crs='epsg:4326').plot(ax=base, color=\"None\", edgecolor='black')\n",
    "    plt.axis(gpdrecords.total_bounds[[0,2,1,3]]+[-0.3, 0.3, -0.1, 0.1])\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job.refresh())\n",
    "consolidater.wait_job(job, wait_secs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Parallel processing of timeseries\n",
    "\n",
    "Data is ready to be retrieved as deep timeseries.\n",
    "<img src=\"images/3.Processing.png\" width=800>\n",
    "\n",
    "The timeseries will be processed on a annual basis and the statistics will be stored in a record named after the date of the year.\n",
    "\n",
    "Two variables are created to define the `Mean` and the `StandardDeviation` as `float32`. To distinguish between annual statistics and others, an instance named `annual` is created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StdVarName = f\"{VariableName}_StandardDeviation\"\n",
    "MeanVarName = f\"{VariableName}_Mean\"\n",
    "RecordYRef = f\"Koln_{YearRef.year}\"\n",
    "\n",
    "# Create variables\n",
    "print(f\"Create variable {StdVarName} and {MeanVarName}\")\n",
    "client.create_variable(StdVarName, \"float32\", [''], exist_ok=True).instantiate('annual', {})\n",
    "client.create_variable(MeanVarName, \"float32\", [''], exist_ok=True).instantiate('annual', {})\n",
    "        \n",
    "# Create records\n",
    "print(f\"Create record {RecordYRef} for the year {YearRef.year}\")\n",
    "aoi = utils.read_aoi(AoiPath)\n",
    "aoi_id = client.create_aoi(aoi, exist_ok=True)\n",
    "r=client.create_record(aoi_id, RecordYRef, {}, YearRef, exist_ok=True)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute statistics of the year of reference\n",
    "`compute_statistics` will be executed on the timeseries of the year of reference and the results will be indexed in the geocube. `compute_and_index` is in charge of this.\n",
    "\n",
    "It takes as parameter: `connection_params` to connect to the client, a `record_name` and other standard parameters in input to be consistent with `sdk.cube_callback_t`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('outputs/'+StdVarName, exist_ok=True)\n",
    "os.makedirs('outputs/'+MeanVarName, exist_ok=True)\n",
    "\n",
    "def compute_and_index(connection_params, timeseries, crs, transform, record):\n",
    "    # Compute statistics\n",
    "    std, mean = compute_statistics(timeseries)\n",
    "    if std is None:\n",
    "        return\n",
    "    \n",
    "    # Index images\n",
    "    client = connection_params.new_client()      \n",
    "    filename = \"_\".join([str(x) for x in transform.to_gdal()])+\".tiff\"\n",
    "    index(client, StdVarName, \"annual\", record, transform, crs, std, os.path.join(os.getcwd(), 'outputs', StdVarName, filename))\n",
    "    index(client, MeanVarName, \"annual\", record, transform, crs, mean, os.path.join(os.getcwd(), 'outputs', MeanVarName, filename))\n",
    "    return \"Indexed !\"\n",
    "    \n",
    "    \n",
    "def index(client, variable, instance, record, transform, crs, image, image_path):\n",
    "    variable = client.variable(variable).instance(instance)\n",
    "    utils.image_to_geotiff(image, transform, crs, variable.dformat.no_data, image_path)\n",
    "    client.index([entities.Container(image_path, True,\n",
    "                                     [entities.Dataset(record, variable)])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the AOI could be very large, `compute_and_index` will be executed on tiles (covering the AOI) and processed using `sdk.get_cube()` (see [Catalogue](Geocube-Client-SDK-1.ipynb/#Catalogue-functions)) in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from geocube import utils, entities\n",
    "\n",
    "aoi   = utils.read_aoi(AoiPath)\n",
    "shape = (256, 256)\n",
    "tiles = client.tile_aoi(aoi, crs=CRS, resolution=Resolution, shape=shape)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "gpd.GeoSeries(aoi, crs='epsg:4326').plot(ax=entities.Tile.plot(tiles), color=\"None\", edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from geocube import sdk\n",
    "\n",
    "# Load the records and the variable\n",
    "from_time = datetime(YearRef.year, 1, 1)\n",
    "to_time   = datetime(YearRef.year, 12, 31)\n",
    "records   = client.list_records(aoi=aoi, from_time=from_time, to_time=to_time, tags=RecordTags)\n",
    "variable  = client.variable(VariableName).instance(InstanceName)\n",
    "recordRef = client.list_records(name=RecordYRef)[0]\n",
    "\n",
    "# For each tile, define the parameters of `sdk.get_cube()`\n",
    "pool_params = {f\"tile{i}\": {\n",
    "    \"connection_params\": connection_params,\n",
    "    \"cube_params\": entities.CubeParams.from_tile(tile=t, records=records, instance=variable),\n",
    "    \"cube_callback\": functools.partial(compute_and_index,\n",
    "                                       connection_params=connection_params,\n",
    "                                       record=recordRef)\n",
    "} for i, t in enumerate(tiles)}\n",
    "\n",
    "print(f\"{len(pool_params)} tasks defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel computing using `dask`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, wait\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "tasks = [dask.delayed(sdk.get_cube)(**params) for params in pool_params.values()]\n",
    "cluster = None\n",
    "\n",
    "try:\n",
    "    cluster = Client(n_workers=2, threads_per_worker=1)\n",
    "    futures = cluster.compute(tasks)\n",
    "\n",
    "    with tqdm(total=len(futures)) as pbar:\n",
    "        while len(futures) > 0:\n",
    "            nf = []\n",
    "            for f in futures:\n",
    "                if f.status==\"finished\":\n",
    "                    pbar.update(1)\n",
    "                else:\n",
    "                    nf.append(f)\n",
    "                    if f.status == \"error\":\n",
    "                        print(\"retry\", f, f.exception())\n",
    "                        f.retry()\n",
    "            time.sleep(0.5)\n",
    "            futures = nf\n",
    "    print(\"done\")\n",
    "finally:\n",
    "    if cluster is not None:\n",
    "        cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize statistics\n",
    "Before last step, we check that the annual statistics computed are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 50]\n",
    "\n",
    "record = client.list_records(RecordYRef)[0]\n",
    "tile = entities.Tile.from_aoi(aoi, crs=CRS, resolution=20)\n",
    "cp = entities.CubeParams.from_tile(tile, records=[record], instance=None)\n",
    "\n",
    "cp.instance = client.variable(MeanVarName).instance(\"annual\")\n",
    "cube_it = client.get_cube_it(cp)\n",
    "plt.subplot(1, 2, 1).set_axis_off()\n",
    "plt.imshow(next(cube_it)[0][...,0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "plt.title(MeanVarName)\n",
    "cube_it.metadata().info()\n",
    "\n",
    "cp.instance = client.variable(StdVarName).instance(\"annual\")\n",
    "images, _ = client.get_cube(cp)\n",
    "plt.subplot(1, 2, 2).set_axis_off()\n",
    "plt.imshow(images[0][...,0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "_ =plt.title(StdVarName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Monitoring abnormal changes\n",
    "As we want to call `abnormal_changes()` on all the 2021 images, several months after the events, the whole timeseries will be retrieved at once.\n",
    "But for a monitoring service, only the newest images are processed. In that case, the consolidation step may not be necessary (at least for the \"timeseries\" perspective).\n",
    "\n",
    "<img src=\"images/4.Monitoring.png\" width=800>\n",
    "\n",
    "The function `abnormal_changes_on_tile` downloads the 2021 timeseries and the 2020 statistics on a given tile, call `abnormal_changes()` for each time step and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def imshowgray(ax, img, title, **kwargs):\n",
    "    if \"vmax\" not in kwargs:\n",
    "        kwargs[\"vmax\"] = 1\n",
    "    ax.imshow(img, cmap='gray', vmin=0, **kwargs)\n",
    "    ax.set_title(title)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "def abnormal_changes_on_tile(connection_params, cube_params, record_ref, z_score_low, z_score_high, min_area):\n",
    "    client = connection_params.new_client()\n",
    "    \n",
    "    # Load statistics on cube_params.Tile\n",
    "    cp = entities.CubeParams.from_tile(cube_params, records=[record_ref], instance=None)\n",
    "    cp.instance = client.variable(MeanVarName).instance(\"annual\")\n",
    "    mean, _ = client.get_cube(cp, verbose=False)\n",
    "\n",
    "    cp.instance = client.variable(StdVarName).instance(\"annual\")\n",
    "    std, _ = client.get_cube(cp, verbose=False)\n",
    "    \n",
    "    nimages = len(cube_params.records)+1\n",
    "    fig, axs = plt.subplots(nimages, 2, constrained_layout=True)\n",
    "    imshowgray(axs[0][0], mean[0][..., 0], \"Mean 2020\")\n",
    "    imshowgray(axs[0][1], std[0][..., 0], \"Standard deviation 2020\")\n",
    "    \n",
    "    for i, (image, metadata, err) in enumerate(client.get_cube_it(cube_params)):\n",
    "        if err is not None:\n",
    "            continue\n",
    "        imshowgray(axs[i+1][0], image[:,:,0], f\"{VariableName} {metadata.min_date}\")\n",
    "        imshowgray(axs[i+1][1], ((mean[0]-image)/std[0])[...,0], f\"Z-Score {metadata.min_date}\", vmax=4)\n",
    "\n",
    "        patches, changes = abnormal_changes(image, mean[0], std[0], z_score_low, z_score_high, min_area)\n",
    "        for patch in patches:\n",
    "            axs[i+1][1].add_patch(Rectangle((patch[0], patch[1]), patch[2], patch[3], edgecolor='r', facecolor='none'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tiling for monitoring can be different from the one for preprocessing (usually, system requirements are different).\n",
    "\n",
    "For instance, monitoring can be done in a webmercator grid at level 12 (resolution~=40 at equador)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 60]\n",
    "recordYRef = client.list_records(name=RecordYRef)[0]\n",
    "\n",
    "# Define the tiling\n",
    "aoi   = utils.read_aoi(AoiPath)\n",
    "layout = entities.Layout.web_mercator(\"\", z_level=12)\n",
    "tiles = client.tile_aoi(aoi, layout=layout)\n",
    "\n",
    "from_time = datetime(YearCur.year, 6, 1)\n",
    "to_time   = datetime(YearCur.year, 7, 31)\n",
    "records = client.list_records(aoi=aoi, from_time=from_time, to_time=to_time, tags=RecordTags)\n",
    "records = entities.Record.group_by(records, entities.Record.key_date)\n",
    "variable = client.variable(VariableName).instance(InstanceName)\n",
    "\n",
    "# Compute the tile where the landslide happened\n",
    "cp = entities.CubeParams.from_tile(tiles[40], records=records, instance=variable)\n",
    "\n",
    "abnormal_changes_on_tile(connection_params, cp, recordYRef, 1.7, 3, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Z-score on the coherence image computed on 21 July (coherence between 15 and 21) shows a lot of abnormal changes. One of the patches is located where the landslide happened (south-west from the center of the image). I don't know if it's really the landslide or something else and, actually, this is not the place to discuss the efficiency of this algorithm :)\n",
    "\n",
    "In fact, the important thing to remember is that the Abnormal Change Detection Algorithm was easily connected to the Geocube, ready to scale-up and the processing was parallelized using the `sdk` package compatible with any multiprocess framework like `dask`.\n",
    "\n",
    "This concludes the Geocube tutorial. I hope this gives you lots of ideas for using your new geocube.\n",
    "\n",
    "Do not hesitate to contact the Geocube team via the [github](https://www.github.com/airbusgeo/) and to have a look at the other projects.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
